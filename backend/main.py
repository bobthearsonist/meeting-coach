"""
Teams Meeting Coach - Main Application (WebSocket Version)
The backend always runs as a WebSocket server broadcasting updates.
The console dashboard is now a WebSocket client that displays the updates.

Usage:
    python main.py              # Starts WebSocket server + integrated engine
    python console_client.py    # Connect console dashboard (separate terminal)
"""
import sys
import time
import threading
import argparse
import warnings
import logging
import os

# Suppress ctranslate2 float16 warnings
os.environ['CT2_VERBOSE'] = '0'
logging.getLogger('ctranslate2').setLevel(logging.ERROR)
warnings.filterwarnings('ignore', message='.*float16.*')
warnings.filterwarnings('ignore', message='.*compute type.*')

from RealtimeSTT import AudioToTextRecorder
from src.core.analyzer import CommunicationAnalyzer
from src.ui.timeline import EmotionalTimeline
from src.ui.colors import colorize_emotional_state, colorize_social_cue, colorize_alert
from src.server.ws_server import MeetingCoachWebSocketServer
from src import config


class MeetingCoach:
    def __init__(self, ws_server: MeetingCoachWebSocketServer, device_index: int = None):
        """
        Initialize the Meeting Coach system with WebSocket broadcasting.

        Args:
            ws_server: WebSocket server instance for broadcasting updates
            device_index: Specific device index to use (for RealtimeSTT)
        """
        print("Initializing Teams Meeting Coach WebSocket Engine...")

        # WebSocket server reference
        self.ws_server = ws_server
        self.session_start_time = time.time()

        # Initialize core components
        self.analyzer = CommunicationAnalyzer()
        self.timeline = EmotionalTimeline(window_minutes=15, max_entries=200)

        # Initialize RealtimeSTT
        print("üé§ Initializing RealtimeSTT recorder...")
        try:
            self.recorder = AudioToTextRecorder(
                model=config.WHISPER_MODEL,
                language="en",

                # Optimized VAD settings for continuous speech detection
                post_speech_silence_duration=0.3,  # Shorter silence before stopping
                min_length_of_recording=0.2,       # Minimum recording length
                min_gap_between_recordings=0.1,    # Smaller gap between recordings
                pre_recording_buffer_duration=0.3, # Buffer before speech starts

                # VAD sensitivity settings
                silero_sensitivity=0.6,             # Moderate sensitivity
                webrtc_sensitivity=2,               # Balanced WebRTC sensitivity

                # Disable features that might interfere
                enable_realtime_transcription=False,
                use_microphone=True,
                spinner=False,

                # Enable debug mode for troubleshooting
                debug_mode=True,

                # Add callbacks for monitoring
                on_recording_start=self._on_recording_start,
                on_recording_stop=self._on_recording_stop,
            )
            print("‚úÖ RealtimeSTT recorder initialized successfully")
        except Exception as e:
            print(f"‚ùå Error initializing RealtimeSTT: {e}")
            raise

        self.is_running = False
        self.is_listening = False
        self.last_wpm = 0

        print(f"‚úÖ Meeting Coach initialized with model: {config.WHISPER_MODEL}")

    def _on_recording_start(self):
        """Callback when RealtimeSTT starts recording"""
        print("üé§ Recording started...")
        self.is_listening = True

        # Broadcast listening status
        self.broadcast_update({
            'type': 'recording_status',
            'is_listening': True
        })

    def _on_recording_stop(self):
        """Callback when RealtimeSTT stops recording"""
        print("üé§ Recording stopped...")
        self.is_listening = False

        # Broadcast listening status
        self.broadcast_update({
            'type': 'recording_status',
            'is_listening': False
        })

    def broadcast_update(self, data: dict):
        """Broadcast update to all connected WebSocket clients"""
        data['timestamp'] = time.time()
        self.ws_server.broadcast_sync(data)

    def process_speech(self, text: str):
        """
        Process complete speech utterances from RealtimeSTT.
        Broadcasts all updates via WebSocket to connected clients.

        Args:
            text: Complete speech utterance detected by RealtimeSTT
        """
        if not text or len(text.strip()) < 3:
            return

        word_count = len(text.split())

        # Calculate speaking pace
        estimated_seconds = max(1.0, word_count * 0.4)  # ~0.4 seconds per word (150 WPM)
        wpm = (word_count / estimated_seconds) * 60 if estimated_seconds > 0 else 0
        wpm = min(max(wpm, 50), 300)  # Cap between 50-300 WPM
        self.last_wpm = wpm

        print(f"üéôÔ∏è Detected speech: \"{text[:60]}{'...' if len(text) > 60 else ''}\" ({word_count} words, ~{wpm:.0f} WPM)")

        # Count filler words
        filler_counts = self._count_filler_words(text)

        # Broadcast transcription immediately
        self.broadcast_update({
            'type': 'transcription',
            'text': text,
            'wpm': wpm,
            'word_count': word_count,
            'filler_counts': filler_counts
        })

        # Check speaking pace for alerts
        pace_feedback = self._get_speaking_pace_feedback(wpm)
        if 'Too' in pace_feedback or 'slow' in pace_feedback.lower():
            self.broadcast_update({
                'type': 'alert',
                'message': pace_feedback,
                'severity': 'warning',
                'category': 'pace'
            })

        # Only analyze if we have enough content
        if word_count >= config.MIN_WORDS_FOR_ANALYSIS:
            print(f"üìù Analyzing: {text[:50]}{'...' if len(text) > 50 else ''} ({word_count} words)")

            # Perform full LLM analysis
            tone_analysis = self.analyzer.analyze_tone(text)

            # Extract analysis fields
            emotional_state = tone_analysis.get('emotional_state', 'neutral')
            social_cues = tone_analysis.get('social_cues', 'appropriate')
            speech_pattern = tone_analysis.get('speech_pattern', 'normal')
            confidence = tone_analysis.get('confidence', 0.0)
            coaching_feedback = tone_analysis.get('coaching_feedback', tone_analysis.get('suggestions', ''))

            # Enhanced alerting for autism/ADHD coaching
            emotional_alert = self.analyzer.should_alert(emotional_state, confidence)
            social_alert = self.analyzer.should_social_cue_alert(social_cues, confidence)

            # Add to timeline
            self.timeline.add_entry(
                emotional_state=emotional_state,
                social_cue=social_cues,
                confidence=confidence,
                text=text,
                alert=emotional_alert or social_alert
            )

            # Broadcast emotional state update
            self.broadcast_update({
                'type': 'emotion_update',
                'emotional_state': emotional_state,
                'confidence': confidence
            })

            # Broadcast full meeting update
            self.broadcast_update({
                'type': 'meeting_update',
                'emotional_state': emotional_state,
                'social_cue': social_cues,
                'confidence': confidence,
                'wpm': wpm,
                'text': text,
                'coaching': coaching_feedback,
                'alert': emotional_alert or social_alert,
                'filler_counts': filler_counts,
                'speech_pattern': speech_pattern
            })

            # Broadcast alerts if needed
            if emotional_alert:
                self.broadcast_update({
                    'type': 'alert',
                    'message': f'Emotional state: {emotional_state.upper()} - {coaching_feedback}',
                    'severity': 'warning',
                    'category': 'emotional',
                    'emotional_state': emotional_state
                })

            if social_alert:
                self.broadcast_update({
                    'type': 'alert',
                    'message': f'Social cue alert: {social_cues}',
                    'severity': 'warning',
                    'category': 'social',
                    'social_cue': social_cues
                })

        else:
            # For short utterances, broadcast basic update
            self.timeline.add_entry(
                emotional_state='neutral',
                social_cue='appropriate',
                confidence=0.0,
                text=text,
                alert=False
            )

            self.broadcast_update({
                'type': 'meeting_update',
                'emotional_state': 'neutral',
                'social_cue': 'appropriate',
                'confidence': 0.0,
                'wpm': wpm,
                'text': text,
                'coaching': '',
                'alert': False,
                'filler_counts': filler_counts
            })

        # Broadcast timeline summary
        self._broadcast_timeline_summary()

    def _broadcast_timeline_summary(self):
        """Broadcast current timeline summary"""
        summary = self.timeline.get_session_summary()
        recent_entries = self.timeline.get_recent_entries(10)

        # Convert timeline entries to serializable format
        timeline_data = []
        for entry in recent_entries:
            timeline_data.append({
                'emotional_state': entry.emotional_state,
                'social_cue': entry.social_cue,
                'confidence': entry.confidence,
                'text': entry.text,
                'alert': entry.alert,
                'timestamp': entry.timestamp
            })

        self.broadcast_update({
            'type': 'timeline_update',
            'summary': summary,
            'recent_entries': timeline_data
        })

    def _get_speaking_pace_feedback(self, wpm: float) -> str:
        """Get speaking pace feedback (replaces transcriber method)"""
        if wpm >= config.PACE_TOO_FAST:
            return "Too fast - slow down for clarity"
        elif wpm <= config.PACE_TOO_SLOW:
            return "Too slow - increase pace slightly"
        elif config.PACE_IDEAL_MIN <= wpm <= config.PACE_IDEAL_MAX:
            return "Perfect pace!"
        else:
            return "Good pace"

    def _count_filler_words(self, text: str) -> dict:
        """Count filler words (replaces transcriber method)"""
        text_lower = text.lower()
        filler_counts = {}

        for filler in config.FILLER_WORDS:
            count = text_lower.count(filler)
            if count > 0:
                filler_counts[filler] = count

        return filler_counts

    def run(self):
        """Start the meeting coach with WebSocket broadcasting."""
        print("üöÄ Starting Meeting Coach WebSocket Engine...")
        print("üé§ Speak naturally - RealtimeSTT will detect speech automatically")
        print("üì° Broadcasting updates to all connected clients")
        print("üí° Open console_client.py in another terminal to see the dashboard")
        print("üõë Press Ctrl+C to stop")

        # Broadcast session start
        self.broadcast_update({
            'type': 'session_status',
            'status': 'started',
            'message': 'Meeting coach session started',
            'config': {
                'whisper_model': config.WHISPER_MODEL,
                'ollama_model': self.analyzer.model
            }
        })

        self.is_running = True
        self.session_start_time = time.time()

        try:
            # Use the continuous RealtimeSTT pattern
            while self.is_running:
                try:
                    # Wait for speech - blocks until complete speech utterance is detected
                    self.recorder.text(self.process_speech)

                    # Brief pause to prevent overwhelming the system
                    time.sleep(0.1)

                except KeyboardInterrupt:
                    break
                except Exception as e:
                    print(f"‚ö†Ô∏è RealtimeSTT error: {e}")
                    self.broadcast_update({
                        'type': 'error',
                        'message': f'Processing error: {str(e)}'
                    })
                    time.sleep(0.5)

        except KeyboardInterrupt:
            print("\n\nüõë Stopping Meeting Coach...")

        finally:
            self.cleanup()

    def cleanup(self):
        """Cleanup resources and broadcast session summary."""
        print("üßπ Starting cleanup...")

        self.is_running = False

        # RealtimeSTT cleanup
        try:
            self.recorder.shutdown()
            print("‚úÖ RealtimeSTT shut down")
        except Exception as e:
            print(f"‚ö†Ô∏è Error shutting down RealtimeSTT: {e}")

        # Broadcast final session summary
        summary = self.timeline.get_session_summary()
        session_duration = time.time() - self.session_start_time

        self.broadcast_update({
            'type': 'session_status',
            'status': 'stopped',
            'message': 'Meeting coach session ended',
            'summary': summary,
            'duration_seconds': session_duration
        })

        print("="*70)
        print("ÔøΩ SESSION COMPLETE")
        print("="*70)
        print(f"üïê Duration: {session_duration/60:.1f} minutes")
        print(f"üìù Total Analyses: {summary.get('total_entries', 0)}")
        print(f"üö® Alerts: {summary.get('alert_count', 0)}")
        if summary.get('dominant_state'):
            dominant = summary['dominant_state']
            dominant_colored = colorize_emotional_state(dominant)
            print(f"ÔøΩ Dominant State: {dominant_colored}")
        print("="*70)


def main():
    """Main entry point - starts WebSocket server with MeetingCoach engine"""
    parser = argparse.ArgumentParser(description="Teams Meeting Coach - WebSocket Server")
    parser.add_argument(
        "--host",
        type=str,
        default="localhost",
        help="WebSocket server host (default: localhost)"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=3001,
        help="WebSocket server port (default: 3001)"
    )
    parser.add_argument(
        "--device",
        type=int,
        help="Audio input device index to use"
    )

    args = parser.parse_args()

    print("üß† Teams Meeting Coach - WebSocket Server")
    print("=" * 70)
    print(f"üì° Starting WebSocket server on ws://{args.host}:{args.port}")
    print(f"üí° Connect clients with: python -m src.server.console_client --url ws://{args.host}:{args.port}")
    print("=" * 70)

    # Create WebSocket server
    ws_server = MeetingCoachWebSocketServer(host=args.host, port=args.port)

    # Create Meeting Coach engine
    coach = MeetingCoach(
        ws_server=ws_server,
        device_index=args.device
    )

    # Run WebSocket server in background thread
    server_thread = threading.Thread(target=ws_server.run, daemon=True)
    server_thread.start()

    # Give server time to start
    time.sleep(2)

    # Run the meeting coach engine (blocking)
    try:
        coach.run()
    except KeyboardInterrupt:
        print("\nüëã Goodbye!")


if __name__ == "__main__":
    main()
