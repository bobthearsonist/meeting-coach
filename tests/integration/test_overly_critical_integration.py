"""
Integration test for overly critical speech analysis
"""
import pytest
import json
from unittest.mock import patch
import sys
import os

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from analyzer import CommunicationAnalyzer


class TestOverlyCriticalIntegration:
    """Integration tests for overly critical speech detection"""
    
    @pytest.fixture
    def analyzer(self):
        """Create analyzer instance for testing."""
        return CommunicationAnalyzer()
    
    @pytest.mark.integration
    def test_end_to_end_overly_critical_detection(self, analyzer):
        """Test complete flow: critical text â†’ analysis â†’ alert â†’ emoji"""
        
        # Test cases with realistic overly critical speech (meeting the minimum word count)
        test_cases = [
            {
                'text': "This code is absolute garbage and completely unworkable. Whoever wrote this obviously has no idea what they're doing and should not be writing code.",
                'description': 'Personal attack on competence'
            },
            {
                'text': "That's the stupidest suggestion I've ever heard in my entire career. You clearly don't understand the problem at all and are wasting everyone's valuable time here.",
                'description': 'Insulting language with dismissal'
            },
            {
                'text': "I can't believe you would waste everyone's time with such a terrible and poorly thought out idea. This is completely wrong and shows poor judgment on your part.",
                'description': 'Harsh criticism with personal judgment'
            }
        ]
        
        # Mock Ollama to simulate realistic overly critical detection
        with patch('analyzer.ollama.generate') as mock_generate:
            mock_generate.return_value = {
                'response': json.dumps({
                    "emotional_state": "overly_critical",
                    "social_cues": "inappropriate", 
                    "speech_pattern": "harsh",
                    "confidence": 0.85,
                    "key_indicators": ["harsh language", "personal attack", "dismissive tone"],
                    "coaching_feedback": "Consider using more constructive language when providing feedback. Focus on the work, not the person."
                })
            }
            
            for case in test_cases:
                # Step 1: Analyze the text
                result = analyzer.analyze_tone(case['text'])
                
                # Step 2: Verify analysis detected overly critical behavior
                emotional_state = result.get('emotional_state', result.get('tone'))
                assert emotional_state == 'overly_critical', \
                    f"Failed to detect overly critical pattern in: {case['description']}"
                
                # Step 3: Verify confidence is reasonable
                confidence = result.get('confidence', 0)
                assert confidence >= 0.7, \
                    f"Confidence should be high for clear overly critical pattern: {case['description']}"
                
                # Step 4: Verify alert is triggered
                should_alert = analyzer.should_alert(emotional_state, confidence)
                assert should_alert == True, \
                    f"Should trigger alert for overly critical behavior: {case['description']}"
                
                # Step 5: Verify correct emoji is returned
                emoji = analyzer.get_tone_emoji(emotional_state)
                assert emoji == 'ðŸ‘Ž', \
                    f"Should return thumbs down emoji for overly critical: {case['description']}"
                
                # Step 6: Verify coaching feedback is provided
                coaching = result.get('coaching_feedback', result.get('suggestions', ''))
                assert coaching and len(coaching) > 10, \
                    f"Should provide meaningful coaching feedback: {case['description']}"
    
    @pytest.mark.integration 
    def test_end_to_end_constructive_feedback_not_flagged(self, analyzer):
        """Test that constructive criticism doesn't get flagged as overly critical"""
        
        constructive_examples = [
            {
                'text': "I have some serious concerns about this technical approach and would like to explain the potential issues I see with this implementation.",
                'expected_state': 'engaged'
            },
            {
                'text': "This implementation has some significant problems that need to be addressed, but I think we can work together to improve it substantially.",
                'expected_state': 'calm'  
            },
            {
                'text': "I respectfully disagree with this proposed solution, but I understand your reasoning completely. Here's an alternative approach that might work better.",
                'expected_state': 'calm'
            }
        ]
        
        # Mock Ollama to simulate constructive feedback detection
        for case in constructive_examples:
            with patch('analyzer.ollama.generate') as mock_generate:
                mock_generate.return_value = {
                    'response': json.dumps({
                        "emotional_state": case['expected_state'],
                        "social_cues": "appropriate",
                        "speech_pattern": "clear", 
                        "confidence": 0.8,
                        "key_indicators": ["respectful", "constructive", "collaborative"],
                        "coaching_feedback": "Continue as you are - good constructive communication"
                    })
                }
                
                result = analyzer.analyze_tone(case['text'])
                
                # Should not be flagged as overly critical
                emotional_state = result.get('emotional_state', result.get('tone'))
                assert emotional_state != 'overly_critical', \
                    f"Constructive feedback incorrectly flagged as overly critical: {case['text']}"
                
                # Should not trigger alert
                should_alert = analyzer.should_alert(emotional_state, result.get('confidence', 0))
                assert should_alert == False, \
                    f"Constructive feedback should not trigger alert: {case['text']}"
    
    @pytest.mark.integration
    def test_overly_critical_with_different_confidence_levels(self, analyzer):
        """Test overly critical detection with various confidence levels"""
        
        test_text = "That's a horrible idea that will never work properly and shows a complete lack of understanding of the fundamental requirements we discussed."
        
        confidence_levels = [0.9, 0.8, 0.7, 0.6, 0.5]
        
        for confidence in confidence_levels:
            with patch('analyzer.ollama.generate') as mock_generate:
                mock_generate.return_value = {
                    'response': json.dumps({
                        "emotional_state": "overly_critical",
                        "social_cues": "inappropriate",
                        "speech_pattern": "harsh",
                        "confidence": confidence,
                        "key_indicators": ["harsh language"],
                        "coaching_feedback": "Consider more constructive language"
                    })
                }
                
                result = analyzer.analyze_tone(test_text)
                emotional_state = result.get('emotional_state', result.get('tone'))
                
                # Verify the analysis result
                assert emotional_state == 'overly_critical'
                assert result.get('confidence') == confidence
                
                # Check alert logic based on confidence threshold
                should_alert = analyzer.should_alert(emotional_state, confidence)
                expected_alert = confidence >= 0.7  # Default threshold
                
                assert should_alert == expected_alert, \
                    f"Alert logic incorrect for confidence {confidence}: expected {expected_alert}, got {should_alert}"
    
    @pytest.mark.integration
    def test_overly_critical_summary_generation(self, analyzer):
        """Test that overly critical behavior appears in analysis summaries"""
        
        # Simulate multiple analyses with overly critical patterns
        mock_analyses = [
            {
                'tone': 'overly_critical',
                'emotional_state': 'overly_critical', 
                'confidence': 0.8,
                'suggestions': 'Use more constructive language'
            },
            {
                'tone': 'calm',
                'emotional_state': 'calm',
                'confidence': 0.7,
                'suggestions': 'Continue as you are'
            },
            {
                'tone': 'overly_critical', 
                'emotional_state': 'overly_critical',
                'confidence': 0.9,
                'suggestions': 'Focus on the work, not the person'
            }
        ]
        
        summary = analyzer.generate_summary(mock_analyses)
        
        # Verify overly critical is captured in summary
        assert summary['dominant_tone'] == 'overly_critical', \
            "Overly critical should be dominant tone in summary"
        
        assert 'overly_critical' in summary['tone_distribution'], \
            "Overly critical should appear in tone distribution"
        
        assert summary['tone_distribution']['overly_critical'] == 2, \
            "Should count 2 overly critical instances"
        
        assert len(summary['key_suggestions']) > 0, \
            "Should include suggestions for overly critical behavior"